---
output: html_document
---

```{r, echo = FALSE}
docdate <- format(Sys.Date(),"%d %b %Y")
```

---
title: "Practical Machine Learning Course Assignment"
author: "Simon Cox"
date: `r docdate`
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, cache = TRUE, include = TRUE)
shhh <- suppressPackageStartupMessages 
shhh(library(caret))
shhh(library(scales))
```

# Executive Summary  
This report presents the outcomes of machine learning models applied to personal fitness device data.
In particular, the Weightlifting Exercise Dataset (available at <http://groupware.les.inf.puc-rio.br/har>) is analysed to create a suitable machine learning model, with the goal of predicting the manner in which the participants performed the dumbbell biceps curl weightlifting exercise.  
  
The final model selected was a **Random Forest** model, which achieved an expected out-of-sample accuracy of 94%.
  
  
# Machine Learning Modelling  
This assignment looks at data collected from six young healthy individuals who were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions:  

* exactly according to the specification (Class A),  
* throwing the elbows to the front (Class B),  
* lifting the dumbbell only halfway (Class C),  
* lowering the dumbbell only halfway (Class D) and   
* throwing the hips to the front (Class E).  

The goal of the assignment is to develop a machine learning model to accurately predict which class the performance of the exercise falls into given a dataset taken from the personal fitness devices.  

The following sections describe how the data was prepared, the models built and the final model selected. For brevity only some of the R code has been included; to see all of the R Code the R markdown file is available from the Git Repository at <https://github.com/Coxy-74/Coursera-Course8>.



## Step 1 - Prepare Data  

The raw data is retrieved using the standard read.csv function.  

```{r get_raw_data}
raw_train <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
raw_eval <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")
```

### Preliminary analysis  

Before going any further, a preliminary analysis of the data is done to identify any fields which are not reliable for feeding into a machine learning model. Any columns that have > 70% of values as "NA" or blank will be ignored. Furthermore, there are some more columns of interest; 'X' which is merely a sequential number as well as 3 timestamp fields. We can safely assume that these will not have a positive impact on the final model and so these will be ignored as well.

```{r strip_blank_cols}
na_pct <- apply(raw_train, 2, function(x) sum(is.na(x) | x == '' ) / length(x))
incl_cols <- na_pct < 0.3
raw_train <- raw_train[,incl_cols]   
raw_train <- raw_train[,-c(1,3:5)]                 # need to remove 'X' and all timestamps
raw_eval <- raw_eval[,incl_cols]
raw_eval <- raw_eval[,-c(1,3:5)]                   # need to remove 'X' and all timestamps  
```

This reduces the number of columns from the original dataset from 160 to 56. This is a much more manageable dataset. 

### Impute missing values

The next stage is to impute any missing values:

```{r impute}
sum(is.na(raw_train)); sum(is.na(raw_eval))
```

We can see that there are no values to impute (which is good news!).


### Create training, testing and validation datasets from original training dataset  

The createDataPartition function of the caret package is used to apply a 60:20:20 split.

```{r splitData}
set.seed(2903)
inTrain <- createDataPartition(y=raw_train$classe, p = 0.6, list = FALSE)
df_train <- raw_train[inTrain,]
df_test <- raw_train[-inTrain,]

set.seed(2903)
inTest <- createDataPartition(y = df_test$classe, p = 0.5, list = FALSE)
df_val <- df_test[-inTest,]
df_test <- df_test[inTest,]
```


### Transform categorical variables into binary variables

The dummyVars function of the caret package is used to apply one-hot-encoding to the categorical variables.
```{r oneHotEncoding}
train_y <- df_train$classe   # save labels for later use
ohe_model <- dummyVars(classe ~ ., data = df_train)
df_train <- data.frame(predict(ohe_model, newdata = df_train))
df_train$classe <- train_y
```

### Examine plots of data

At this point we can examine the data and start looking to see if there are any meaningful relationships. The featureplot function can be used to generate a series of boxplots which can show those features which may be important in determining the appropriate label (i.e. the "classe" variable). Only those features which indicate importance are shown below:

```{r featurePlot, echo = TRUE}
featurePlot(x = df_train[, c(9,10,12,13,18,19,30,33:39,43:51,56,59:61)],
            y = df_train$classe, 
            plot = "box",
            title = "Figure 1 - Feature Box Plots",
            strip=strip.custom(par.strip.text=list(cex=.7)),
            scales = list(x = list(relation="free"), 
            y = list(relation="free")))
```

This indicates that we may expect the features relating to "arm", "forearm", "dumbell" and "belt" measurements to be of importance in the model.

We can also look at density plots. Here we are looking for features where the density plots are different shapes, and we apply density plots to the same variables that were highlighted as being important from the analysis of the box plots above. 

```{r featureDensity, echo = TRUE}
featurePlot(x = df_train[, c(9,10,12,13,18,19,30,33:39,43:51,56,59:61)], 
            y = df_train$classe, 
            plot = "density",
            strip=strip.custom(par.strip.text=list(cex=.7)),
            scales = list(x = list(relation="free"), 
                          y = list(relation="free")))
```

We can see that in some of these the density plots are quite different, but for others they are almost identical. As such no clear conclusions can be drawn about the importance of the variables at this stage.

### Correlation  

One thing that is obvious from the previous figures is that there are a lot of potentially important variables. It may be that many of these variables are correlated in some way, so some analysis of correlation is required. The following code was executed to determine how many pairs of features have a correlation of > 0.8 in absolute terms:

```{r correlation}
M <- abs(cor(df_train[,-62]))
diag(M) <- 0
length(which(M > 0.8, arr.ind = T)) /2
```

As we can see, and not unexpectedly, there are a large number of correlated features.


### Principal Component Analysis (PCA)

Such a high degree of correlation amongst so many variables is a concern as it may lead to increased variability in the final model. However we know that removing important features may also lead to an increase in bias in the final model. Therefore in this case, PCA may be a good solution, where principal components comprising weighted averages of features are created. These can then be used as the features for the model.

Unfortunately this does reduce the interpretability of the final model but in this case we are more concerned with the accuracy of the model than the ability to interpret it.  

The preProcess function in caret can perform the PCA for us; a value of 10 principal components has been chosen to try and ensure we retain as much information as reasonably necessary:

```{r PCA}
preProcPCA <- preProcess(df_train,method='pca', pcaComp=10)
pca_train <- predict(preProcPCA, newdata = df_train[,-62])      # exclude the label
pca_train$classe <- train_y                                     # add label back into dataset
```

Note that by default the preProcess function has also scaled and centred our data, so the data preparation is now complete.

### Prepare the test and validation datasets  
Now that we have completed our data preparation on the training dataset, we need to apply the same preparation to our test and validation datasets:

```{r test_val_data}
test_y <- df_test$classe
df_test <- data.frame(predict(ohe_model, newdata = df_test))
pca_test <- predict(preProcPCA, newdata = df_test)
pca_test$classe <- test_y

val_y <- df_val$classe
df_val <- data.frame(predict(ohe_model, newdata = df_val))
pca_val <- predict(preProcPCA, newdata = df_val)
pca_val$classe <- val_y
```


## Step 2 - Train the Models  
The caret function in R will be used to train various models using various methods which will be initially scored using the test dataset. It is noted that caret does a number of things automatically when training a model:  

* Cross-validation of the model  
* Tuning of hyper parameters for optimal model performance
* Choosing of the optimal model based on evaluation metric

The first thing to do is to set up a training control object which all models will be trained with. This will simply define that cross validation will be performed, with k = 5.  Next, we determine our evaluation metric - in this case Accuracy is the most important and as this is the default behaviour of caret no additional coding is needed when training the models.

```{r training_control}
mod_trControl <- trainControl(method = "cv", number = 5)
```

The next step is to train the model using various methods. The first method considered was Naive Bayes (NB). The train function was used to build the model as follows:

```{r train_nb}
set.seed(2903)
modFitnb <- train(classe ~ .
                  , data = pca_train
                  , method = 'nb'
                  , trControl = mod_trControl
                  , tuneLength = 5)
modFitnb 
```

This does not bode well, with relatively low Accuracy and Kappa values. To get a better feel for the out-of-sample accuracy this is applied against the test dataset.

```{r test_nb}
pred_test_nb <- predict(modFitnb,newdata = pca_test)
cm_nb <- confusionMatrix(pred_test_nb, test_y)    
cm_nb
```

We can see that the Accuracy is `r percent(cm_nb$overall[1])` and Kappa is `r percent(cm_nb$overall[2])`. This is not a very accurate model.


### Model Training Summary

Aside from Naive Bayes, several other methods were used to generate different models. A list of all models attempted are shown below:  

* Naive Bayes (NB)  
* Support Vector Machines (SVM)  
* Stochastic Gradient Boosting Machine (GBM)  
* Random Forest (RF)  
* Extreme Gradient Boosting (XGB)  
  
The same code was executed to generate each model except the name of the method was changed from "nb" to the relevant value.  

```{r train_svm, echo = FALSE, include = FALSE}
set.seed(2903)
modFitsvm <- train(classe ~ .
                   , data = pca_train
                   , method = 'svmRadial'
                   , trControl = mod_trControl
                   , tuneLength = 5)
                   
pred_test_svm <- predict(modFitsvm, newdata = pca_test)
cm_svm <- confusionMatrix(pred_test_svm, test_y)    
```

```{r train_gbm, echo = FALSE, include = FALSE}
set.seed(2903)
modFitgbm <- train(classe ~ .
                   , data = pca_train
                   , method = 'gbm'
                   , trControl = mod_trControl
                   , tuneLength = 5
                   , verbose = FALSE)

pred_test_gbm <- predict(modFitgbm,newdata = pca_test)
cm_gbm <- confusionMatrix(pred_test_gbm, test_y)    
```

```{r train_rf, echo = FALSE, include = FALSE}
set.seed(2903)
modFitrf <- train(classe ~ .
                  , data = pca_train
                  , method = 'rf'
                  , trControl = mod_trControl
                  , tuneLength = 5)

pred_test_rf <- predict(modFitrf,newdata = pca_test)
cm_rf <- confusionMatrix(pred_test_rf, test_y)  
```

```{r train_xgb, echo = FALSE, include = FALSE}
set.seed(2903)
modFitxgb <- train(classe ~ .
                   , data = pca_train
                   , method = 'xgbTree'
                   , trControl = mod_trControl)

pred_test_xgb <- predict(modFitxgb,newdata = pca_test)
cm_xgb <- confusionMatrix(pred_test_xgb, test_y)
```

```{r get_training_metrics, echo = FALSE, include = FALSE}
pred_train_nb <- predict(modFitnb,newdata = pca_train)
cm_nbt <- confusionMatrix(pred_train_nb, train_y)

pred_train_svm <- predict(modFitsvm,newdata = pca_train)
cm_svmt <- confusionMatrix(pred_train_svm, train_y)

pred_train_gbm <- predict(modFitgbm,newdata = pca_train)
cm_gbmt <- confusionMatrix(pred_train_gbm, train_y)

pred_train_rf <- predict(modFitrf,newdata = pca_train)
cm_rft <- confusionMatrix(pred_train_rf, train_y)

pred_train_xgb <- predict(modFitxgb,newdata = pca_train)
cm_xgbt <- confusionMatrix(pred_train_xgb, train_y)

```

The results are below:  

|Model    | Training Accuracy  |  Training Kappa   | Test Accuracy  |  Test Kappa   |
|:--------|:------------------:|:-----------------:|:--------------:|:-------------:|
|NB       |`r percent(cm_nbt$overall[1])` |`r percent(cm_nbt$overall[2])` |`r percent(cm_nb$overall[1])` |`r percent(cm_nb$overall[2])` |
|SVM      |`r percent(cm_svmt$overall[1])` |`r percent(cm_svmt$overall[2])` |`r percent(cm_svm$overall[1])`|`r percent(cm_svm$overall[2])`|
|GBM      |`r percent(cm_gbmt$overall[1])` |`r percent(cm_gbmt$overall[2])` |`r percent(cm_gbm$overall[1])`|`r percent(cm_gbm$overall[2])`|
|RF       |`r percent(cm_rft$overall[1])` |`r percent(cm_rft$overall[2])` |`r percent(cm_rf$overall[1])` |`r percent(cm_rf$overall[2])` |
|XGB      |`r percent(cm_xgbt$overall[1])` |`r percent(cm_xgbt$overall[2])` |`r percent(cm_xgb$overall[1])`|`r percent(cm_xgb$overall[2])`|

It is clear from the table above that some models performed much better than others. The high training Accuracy and Kappa values for the Random Forest model indicate that there may be overfitting, but the results against the test set are still very good.

## Step 3 - Select the Appropriate Model and Evaluate

From the results attained when applying the different models to the test dataset, the **Random Forest** model produced the best outcome with the highest Accuracy rate. However it is noted that the test dataset was used in coming to this conclusion (i.e. we applied all models to the test dataset to determine which model we will select), therefore we would like to get a further independent view of the expected out-of-sample results. This is where we can use the validation dataset which up until this point has not been used at all.

From the table above, the Accuracy was `r percent(cm_rf$overall[1])` and Kappa was `r percent(cm_rf$overall[2])` when the model was applied to the Test dataset. 

We can now apply the model to the validation dataset and show the full confusion matrix:

```{r predict_val, echo = TRUE}
pred_val <- predict(modFitrf,newdata = pca_val)
cm_val <- confusionMatrix(pred_val, val_y)     
cm_val
```

Our final results:  

* Accuracy `r percent(cm_val$overall[1])`
* Kappa `r percent(cm_val$overall[2])`